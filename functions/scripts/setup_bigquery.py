#!/usr/bin/env python3
"""
Setup script for BigQuery tables for Veritas AI diligence system
Creates dataset and tables if they don't exist
"""

from google.cloud import bigquery
from google.api_core import exceptions
import json

# Configuration
PROJECT_ID = "veritas-472301"
DATASET_ID = "veritas_pitch_data"
DATASET_LOCATION = "asia-south1"

def create_bigquery_client():
    """Create BigQuery client"""
    try:
        client = bigquery.Client(project=PROJECT_ID)
        print(f"‚úÖ BigQuery client initialized for project: {PROJECT_ID}")
        return client
    except Exception as e:
        print(f"‚ùå Error creating BigQuery client: {e}")
        return None

def create_dataset(client):
    """Create dataset if it doesn't exist"""
    dataset_id = f"{PROJECT_ID}.{DATASET_ID}"
    
    try:
        # Check if dataset exists
        client.get_dataset(dataset_id)
        print(f"‚úÖ Dataset '{DATASET_ID}' already exists")
        return True
        
    except exceptions.NotFound:
        print(f"üöÄ Creating dataset '{DATASET_ID}'...")
        
        try:
            dataset = bigquery.Dataset(dataset_id)
            dataset.location = DATASET_LOCATION
            dataset.description = "Veritas AI pitch deck and diligence data"
            
            dataset = client.create_dataset(dataset, timeout=30)
            print(f"‚úÖ Dataset '{DATASET_ID}' created successfully in {DATASET_LOCATION}")
            return True
            
        except Exception as e:
            print(f"‚ùå Error creating dataset: {e}")
            return False

def create_diligence_reports_table(client):
    """Create diligence_reports table"""
    table_id = f"{PROJECT_ID}.{DATASET_ID}.diligence_reports"
    
    try:
        # Check if table exists
        client.get_table(table_id)
        print(f"‚úÖ Table 'diligence_reports' already exists")
        return True
        
    except exceptions.NotFound:
        print(f"üöÄ Creating table 'diligence_reports'...")
        
        schema = [
            bigquery.SchemaField("report_id", "STRING", mode="REQUIRED", description="Unique report identifier"),
            bigquery.SchemaField("company_id", "STRING", mode="REQUIRED", description="Company/memo ID"),
            bigquery.SchemaField("investor_email", "STRING", mode="REQUIRED", description="Investor who ran diligence"),
            bigquery.SchemaField("validation_results", "JSON", mode="NULLABLE", description="Full validation results in JSON"),
            bigquery.SchemaField("processing_time_seconds", "FLOAT", mode="NULLABLE", description="Time taken to process"),
            bigquery.SchemaField("contradictions_count", "INTEGER", mode="NULLABLE", description="Number of contradictions found"),
            bigquery.SchemaField("discrepancies_count", "INTEGER", mode="NULLABLE", description="Number of discrepancies found"),
            bigquery.SchemaField("overall_score", "FLOAT", mode="NULLABLE", description="Overall validation score 0-10"),
            bigquery.SchemaField("status", "STRING", mode="NULLABLE", description="Report status"),
            bigquery.SchemaField("timestamp", "TIMESTAMP", mode="REQUIRED", description="When report was generated"),
        ]
        
        try:
            table = bigquery.Table(table_id, schema=schema)
            table.description = "Diligence validation reports generated by investors"
            
            # Add clustering for better query performance
            table.clustering_fields = ["company_id", "investor_email"]
            
            table = client.create_table(table)
            print(f"‚úÖ Table 'diligence_reports' created successfully")
            return True
            
        except Exception as e:
            print(f"‚ùå Error creating table: {e}")
            return False

def verify_pitch_deck_data_table(client):
    """Verify pitch_deck_data table exists (should have been created earlier)"""
    table_id = f"{PROJECT_ID}.{DATASET_ID}.pitch_deck_data"
    
    try:
        table = client.get_table(table_id)
        print(f"‚úÖ Table 'pitch_deck_data' exists and is ready")
        return True
        
    except exceptions.NotFound:
        print(f"‚ö†Ô∏è  Table 'pitch_deck_data' not found")
        print(f"   This table should have been created by the main system")
        print(f"   Creating it now with basic schema...")
        
        schema = [
            bigquery.SchemaField("upload_id", "STRING", mode="REQUIRED"),
            bigquery.SchemaField("founder_email", "STRING", mode="REQUIRED"),
            bigquery.SchemaField("upload_timestamp", "TIMESTAMP", mode="REQUIRED"),
            bigquery.SchemaField("original_filename", "STRING", mode="NULLABLE"),
            bigquery.SchemaField("memo_1", "JSON", mode="NULLABLE"),
            bigquery.SchemaField("processing_time_seconds", "FLOAT", mode="NULLABLE"),
            bigquery.SchemaField("status", "STRING", mode="NULLABLE"),
        ]
        
        try:
            table = bigquery.Table(table_id, schema=schema)
            table.description = "Pitch deck ingestion and memo generation data"
            table.clustering_fields = ["founder_email"]
            
            table = client.create_table(table)
            print(f"‚úÖ Table 'pitch_deck_data' created successfully")
            return True
            
        except Exception as e:
            print(f"‚ùå Error creating table: {e}")
            return False

def print_summary():
    """Print setup summary"""
    print("\n" + "="*70)
    print("üéâ BIGQUERY SETUP COMPLETE!")
    print("="*70)
    
    print(f"\nüìã DATASET INFORMATION:")
    print(f"   Project: {PROJECT_ID}")
    print(f"   Dataset: {DATASET_ID}")
    print(f"   Location: {DATASET_LOCATION}")
    
    print(f"\nüìã TABLES CREATED:")
    print(f"   1. {DATASET_ID}.pitch_deck_data")
    print(f"      - Stores pitch deck uploads and memo1 data")
    print(f"   2. {DATASET_ID}.diligence_reports")
    print(f"      - Stores diligence validation reports")
    
    print("\n" + "="*70)
    print("‚úÖ Next Steps:")
    print("="*70)
    print("1. BigQuery is now ready for use")
    print("2. The system will automatically save data to these tables")
    print("3. You can query data using BigQuery console or SQL")
    print("4. Run setup_vector_search.py if not done already")
    print("="*70 + "\n")

def main():
    """Main setup function"""
    print("\n" + "="*70)
    print("üîß VERITAS AI - BIGQUERY SETUP")
    print("="*70 + "\n")
    
    # Create client
    client = create_bigquery_client()
    if not client:
        print("\n‚ùå Setup failed. Could not create BigQuery client.")
        return
    
    # Create dataset
    if not create_dataset(client):
        print("\n‚ùå Setup failed. Could not create dataset.")
        return
    
    # Verify/create pitch_deck_data table
    print(f"\nüîç Checking pitch_deck_data table...")
    verify_pitch_deck_data_table(client)
    
    # Create diligence_reports table
    print(f"\nüîç Checking diligence_reports table...")
    create_diligence_reports_table(client)
    
    # Print summary
    print_summary()

if __name__ == "__main__":
    main()

